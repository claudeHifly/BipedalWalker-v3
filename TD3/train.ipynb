{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "train.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "o6eaVN38QdnT",
    "colab_type": "code",
    "outputId": "4ccf7a08-01cf-4b1d-a688-ea6139a38fb3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1591645977849,
     "user_tz": -120,
     "elapsed": 11566,
     "user": {
      "displayName": "CLAUDIO CIAVOLA",
      "photoUrl": "https://lh3.googleusercontent.com/-KNU432Py9wA/AAAAAAAAAAI/AAAAAAAADvs/HFBxd4uSnlM/s64/photo.jpg",
      "userId": "08727435020611235754"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    }
   },
   "source": [
    "!pip install gym\n",
    "!pip install box2d_py"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
      "Collecting box2d_py\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
      "\u001B[K     |████████████████████████████████| 450kB 2.8MB/s \n",
      "\u001B[?25hInstalling collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.8\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cCy2ModEPxoV",
    "colab_type": "code",
    "outputId": "3f674475-cc20-4502-d0e0-13749df2712d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1591646005578,
     "user_tz": -120,
     "elapsed": 39284,
     "user": {
      "displayName": "CLAUDIO CIAVOLA",
      "photoUrl": "https://lh3.googleusercontent.com/-KNU432Py9wA/AAAAAAAAAAI/AAAAAAAADvs/HFBxd4uSnlM/s64/photo.jpg",
      "userId": "08727435020611235754"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0nZfVOUiP0CO",
    "colab_type": "code",
    "outputId": "13d18ea8-1732-498f-cf78-2308b19a1751",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1591646085034,
     "user_tz": -120,
     "elapsed": 1181,
     "user": {
      "displayName": "CLAUDIO CIAVOLA",
      "photoUrl": "https://lh3.googleusercontent.com/-KNU432Py9wA/AAAAAAAAAAI/AAAAAAAADvs/HFBxd4uSnlM/s64/photo.jpg",
      "userId": "08727435020611235754"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "%cd /content/drive/My Drive/BipedalWalker-v3/TD3"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1TyolH62paiFvrPtkZ3ZJunv4rqrxh7Nz/progettoDataDriven/gym_BipedalWalker-v3/TD3/6-TD3-PyTorch-doddo22\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jxxoDia4PjTN",
    "colab_type": "code",
    "outputId": "4b9df984-2a8f-45c6-8ee8-60f7015fd006",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1591683404974,
     "user_tz": -120,
     "elapsed": 30217104,
     "user": {
      "displayName": "CLAUDIO CIAVOLA",
      "photoUrl": "https://lh3.googleusercontent.com/-KNU432Py9wA/AAAAAAAAAAI/AAAAAAAADvs/HFBxd4uSnlM/s64/photo.jpg",
      "userId": "08727435020611235754"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    }
   },
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from TD3 import TD3\n",
    "from utils import ReplayBuffer\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "\n",
    "######### Hyperparameters #########\n",
    "gym.logger.set_level(40)\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "log_interval = 100  # print avg reward after interval\n",
    "random_seed = 0\n",
    "gamma = 0.99  # discount for future rewards\n",
    "batch_size = 100  # num of transitions sampled from replay buffer\n",
    "lr = 0.001\n",
    "exploration_noise = 0.1\n",
    "polyak = 0.995  # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2  # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2  # delayed policy updates parameter\n",
    "max_episodes = 10000  # max num of episodes\n",
    "max_timesteps = 2000  # max timesteps in one episode\n",
    "directory = \"./preTrained/\"  # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "\n",
    "start_episode = 0\n",
    "\n",
    "\n",
    "policy = TD3(lr, state_dim, action_dim, max_action)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    env.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "LOAD = False\n",
    "if LOAD:\n",
    "    start_episode = 6\n",
    "    policy.load(directory, filename, str(start_episode))\n",
    "\n",
    "# logging variables:\n",
    "scores = []\n",
    "mean_scores = []\n",
    "last_scores = deque(maxlen=log_interval)\n",
    "distances = []\n",
    "mean_distances = []\n",
    "last_distance = deque(maxlen=log_interval)\n",
    "losses_mean_episode = []\n",
    "\n",
    "# training procedure:\n",
    "for ep in range(start_episode + 1, max_episodes + 1):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    total_distance = 0\n",
    "    actor_losses = []\n",
    "    c1_losses = []\n",
    "    c2_losses = []\n",
    "    for t in range(max_timesteps):\n",
    "        # select action and add exploration noise:\n",
    "        action = policy.select_action(state)\n",
    "        action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "        action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "        # take action in env:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "        if reward != -100:\n",
    "            total_distance += reward\n",
    "\n",
    "        # if episode is done then update policy:\n",
    "        if done or t == (max_timesteps - 1):\n",
    "            actor_loss, c1_loss, c2_loss = policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
    "            actor_losses.append(actor_loss)\n",
    "            c1_losses.append((c1_loss))\n",
    "            c2_losses.append(c2_loss)\n",
    "            break\n",
    "    mean_loss_actor = np.mean(actor_losses)\n",
    "    mean_loss_c1 = np.mean(c1_losses)\n",
    "    mean_loss_c2 = np.mean(c2_losses)\n",
    "    losses_mean_episode.append((ep, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
    "    print('\\rEpisode: {}/{},\\tScore: {:.2f},\\tDistance: {:.2f},\\tactor_loss: {},\\tc1_loss:{},\\tc2_loss:{}'\n",
    "        .format(ep, max_episodes,total_reward,total_distance,mean_loss_actor,mean_loss_c1, mean_loss_c2),end=\"\")\n",
    "\n",
    "    # logging updates:\n",
    "    scores.append(total_reward)\n",
    "    distances.append(total_distance)\n",
    "    last_scores.append(total_reward)\n",
    "    last_distance.append(total_distance)\n",
    "    mean_score = np.mean(last_scores)\n",
    "    mean_distance = np.mean(last_distance)\n",
    "    FILE = 'record.dat'\n",
    "    data = [ep, total_reward, total_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
    "    with open(FILE, \"ab\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    # if avg reward > 300 then save and stop traning:\n",
    "    if (mean_score) >= 300:\n",
    "        print(\"########## Solved! ###########\")\n",
    "        name = filename + '_solved'\n",
    "        policy.save(directory, name, str(ep))\n",
    "        break\n",
    "\n",
    "    # print avg reward every log interval:\n",
    "    if ep % log_interval == 0:\n",
    "        policy.save(directory, filename, str(ep))\n",
    "        mean_scores.append(mean_score)\n",
    "        mean_distances.append(mean_distance)\n",
    "        print('\\rEpisode: {}/{},\\tMean Score: {:.2f},\\tMean Distance: {:.2f},\\tactor_loss: {},\\tc1_loss:{},\\tc2_loss:{}'\n",
    "            .format(ep, max_episodes, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
    "        FILE = 'record_mean.dat'\n",
    "        data = [ep, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
    "        with open(FILE, \"ab\") as f:\n",
    "            pickle.dump(data, f)\n",
    "env.close()"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Episode: 100/10000,\tMean Score: -120.23,\tMean Distance: -57.23,\tactor_loss: 8.303149223327637,\tc1_loss:0.22861288487911224,\tc2_loss:0.8126792311668396\n",
      "Episode: 200/10000,\tMean Score: -22.50,\tMean Distance: 20.50,\tactor_loss: 4.10565185546875,\tc1_loss:0.2367175817489624,\tc2_loss:0.12578314542770386\n",
      "Episode: 300/10000,\tMean Score: 226.14,\tMean Distance: 242.14,\tactor_loss: -10.152022361755371,\tc1_loss:0.7477027773857117,\tc2_loss:0.7801839709281921\n",
      "Episode: 400/10000,\tMean Score: 244.14,\tMean Distance: 262.14,\tactor_loss: -16.728351593017578,\tc1_loss:0.4752908945083618,\tc2_loss:0.8547260761260986\n",
      "Episode: 500/10000,\tMean Score: 284.65,\tMean Distance: 286.65,\tactor_loss: -18.014184951782227,\tc1_loss:0.08428995311260223,\tc2_loss:0.07459189742803574\n",
      "Episode: 600/10000,\tMean Score: 276.57,\tMean Distance: 284.57,\tactor_loss: -26.119003295898438,\tc1_loss:0.6132750511169434,\tc2_loss:1.6637133359909058\n",
      "Episode: 700/10000,\tMean Score: 285.35,\tMean Distance: 292.35,\tactor_loss: -31.020204544067383,\tc1_loss:0.07291256636381149,\tc2_loss:0.06301432102918625\n",
      "Episode: 800/10000,\tMean Score: 281.86,\tMean Distance: 288.86,\tactor_loss: -31.114633560180664,\tc1_loss:20.505788803100586,\tc2_loss:21.236055374145508\n",
      "Episode: 900/10000,\tMean Score: 274.16,\tMean Distance: 284.16,\tactor_loss: -33.62522888183594,\tc1_loss:0.12784436345100403,\tc2_loss:0.08734948933124542\n",
      "Episode: 1000/10000,\tMean Score: 289.88,\tMean Distance: 293.88,\tactor_loss: -31.676502227783203,\tc1_loss:0.37097489833831787,\tc2_loss:0.5291808247566223\n",
      "Episode: 1073/10000,\tScore: 310.04,\tDistance: 310.04,\tactor_loss: -32.92710494995117,\tc1_loss:12.026217460632324,\tc2_loss:12.051828384399414########## Solved! ###########\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}
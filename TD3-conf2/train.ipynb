{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "train.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "o6eaVN38QdnT",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592379932628,
     "user_tz": -120,
     "elapsed": 7773,
     "user": {
      "displayName": "CLAUDIO CIAVOLA",
      "photoUrl": "https://lh3.googleusercontent.com/-KNU432Py9wA/AAAAAAAAAAI/AAAAAAAADvs/HFBxd4uSnlM/s64/photo.jpg",
      "userId": "08727435020611235754"
     }
    },
    "outputId": "9785b0d9-9f73-498d-a2d3-83b7de0ce7bd"
   },
   "source": [
    "!pip install gym\n",
    "!pip install box2d_py"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
      "Collecting box2d_py\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
      "\u001B[K     |████████████████████████████████| 450kB 2.8MB/s \n",
      "\u001B[?25hInstalling collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.8\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cCy2ModEPxoV",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592379951970,
     "user_tz": -120,
     "elapsed": 27100,
     "user": {
      "displayName": "CLAUDIO CIAVOLA",
      "photoUrl": "https://lh3.googleusercontent.com/-KNU432Py9wA/AAAAAAAAAAI/AAAAAAAADvs/HFBxd4uSnlM/s64/photo.jpg",
      "userId": "08727435020611235754"
     }
    },
    "outputId": "c4101b1e-7d03-4350-adce-17ac8181b175"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0nZfVOUiP0CO",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1592379952964,
     "user_tz": -120,
     "elapsed": 28089,
     "user": {
      "displayName": "CLAUDIO CIAVOLA",
      "photoUrl": "https://lh3.googleusercontent.com/-KNU432Py9wA/AAAAAAAAAAI/AAAAAAAADvs/HFBxd4uSnlM/s64/photo.jpg",
      "userId": "08727435020611235754"
     }
    },
    "outputId": "748a10b3-d9e0-473b-9576-627a67b4cffc"
   },
   "source": [
    "%cd /content/drive/My Drive/progettoDataDriven/gym_BipedalWalker-v3/TD3-conf2"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1TyolH62paiFvrPtkZ3ZJunv4rqrxh7Nz/progettoDataDriven/gym_BipedalWalker-v3/TD3/6-TD3-PyTorch-doddo22\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jxxoDia4PjTN",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "outputId": "7c57192e-0eab-4ecf-9b75-09e8c5932d56"
   },
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from TD3 import TD3\n",
    "from utils import ReplayBuffer\n",
    "from collections import deque\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "######### Hyperparameters #########\n",
    "gym.logger.set_level(40)\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "log_interval = 100  # print avg reward after interval\n",
    "random_seed = 0\n",
    "gamma = 0.99  # discount for future rewards\n",
    "batch_size = 100  # num of transitions sampled from replay buffer\n",
    "lr = 0.001\n",
    "exploration_noise = 0.1\n",
    "polyak = 0.995  # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2  # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2  # delayed policy updates parameter\n",
    "max_episodes = 10000  # max num of episodes\n",
    "max_timesteps = 2000  # max timesteps in one episode\n",
    "directory = \"./preTrained/\"  # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "\n",
    "start_episode = 0\n",
    "\n",
    "\n",
    "policy = TD3(lr, state_dim, action_dim, max_action)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    env.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "LOAD = True\n",
    "if LOAD:\n",
    "    start_episode = 900\n",
    "    policy.load(directory, filename, str(start_episode))\n",
    "\n",
    "# logging variables:\n",
    "scores = []\n",
    "mean_scores = []\n",
    "last_scores = deque(maxlen=log_interval)\n",
    "distances = []\n",
    "mean_distances = []\n",
    "last_distance = deque(maxlen=log_interval)\n",
    "losses_mean_episode = []\n",
    "\n",
    "# training procedure:\n",
    "for ep in range(start_episode + 1, max_episodes + 1):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    total_distance = 0\n",
    "    actor_losses = []\n",
    "    c1_losses = []\n",
    "    c2_losses = []\n",
    "    for t in range(max_timesteps):\n",
    "        # select action and add exploration noise:\n",
    "        action = policy.select_action(state)\n",
    "        action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "        action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "        # take action in env:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "        if reward != -100:\n",
    "            total_distance += reward\n",
    "\n",
    "        # if episode is done then update policy:\n",
    "        if done or t == (max_timesteps - 1):\n",
    "            actor_loss, c1_loss, c2_loss = policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
    "            actor_losses.append(actor_loss)\n",
    "            c1_losses.append((c1_loss))\n",
    "            c2_losses.append(c2_loss)\n",
    "            break\n",
    "    mean_loss_actor = np.mean(actor_losses)\n",
    "    mean_loss_c1 = np.mean(c1_losses)\n",
    "    mean_loss_c2 = np.mean(c2_losses)\n",
    "    losses_mean_episode.append((ep, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
    "    print('\\rEpisode: {}/{},\\tScore: {:.2f},\\tDistance: {:.2f},\\tactor_loss: {},\\tc1_loss:{},\\tc2_loss:{}'\n",
    "        .format(ep, max_episodes,total_reward,total_distance,mean_loss_actor,mean_loss_c1, mean_loss_c2),end=\"\")\n",
    "\n",
    "    # logging updates:\n",
    "    scores.append(total_reward)\n",
    "    distances.append(total_distance)\n",
    "    last_scores.append(total_reward)\n",
    "    last_distance.append(total_distance)\n",
    "    mean_score = np.mean(last_scores)\n",
    "    mean_distance = np.mean(last_distance)\n",
    "    FILE = 'record.dat'\n",
    "    data = [ep, total_reward, total_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
    "    with open(FILE, \"ab\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    # if avg reward > 300 then save and stop traning:\n",
    "    # end = True\n",
    "    # for s in last_scores:\n",
    "    #     if s < 300:\n",
    "    #         end = False\n",
    "    #         break\n",
    "    # if end and len(last_scores) == 100:\n",
    "    #     print(\"########## Solved! ###########\")\n",
    "    #     name = filename + '_solved'\n",
    "    #     policy.save(directory, name, str(ep))\n",
    "    #     break\n",
    "\n",
    "    # print avg reward every log interval:\n",
    "    if ep % log_interval == 0:\n",
    "        policy.save(directory, filename, str(ep))\n",
    "        mean_scores.append(mean_score)\n",
    "        mean_distances.append(mean_distance)\n",
    "        print('\\rEpisode: {}/{},\\tMean Score: {:.2f},\\tMean Distance: {:.2f},\\tactor_loss: {},\\tc1_loss:{},\\tc2_loss:{}'\n",
    "            .format(ep, max_episodes, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
    "        FILE = 'record_mean.dat'\n",
    "        data = [ep, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
    "        with open(FILE, \"ab\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        if mean_score >= 300 and len(last_scores) == 100:\n",
    "            print(\"########## Solved! ###########\")\n",
    "            name = filename + '_solved'\n",
    "            policy.save(directory, name, str(ep))\n",
    "            break\n",
    "env.close()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Episode: 1000/10000,\tMean Score: 206.35,\tMean Distance: 232.35,\tactor_loss: -22.476764678955078,\tc1_loss:0.7152127623558044,\tc2_loss:0.38814491033554077\n",
      "Episode: 1016/10000,\tScore: 295.23,\tDistance: 295.23,\tactor_loss: -26.685546875,\tc1_loss:0.8223543763160706,\tc2_loss:0.6647594571113586"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}